{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d19f35",
   "metadata": {},
   "source": [
    "# Unit 1 Benchmark: Model Architecture Limitations\n",
    "\n",
    "## Objective\n",
    "Investigate the architectural limitations of different model types (BERT, RoBERTa, BART) by forcing them to perform tasks they may not be designed for.\n",
    "\n",
    "### Models to Test\n",
    "1. **BERT** (`bert-base-uncased`): Encoder-only model (designed for understanding, not generation)\n",
    "2. **RoBERTa** (`roberta-base`): Optimized Encoder-only model\n",
    "3. **BART** (`facebook/bart-base`): Encoder-Decoder model (designed for seq2seq tasks)\n",
    "\n",
    "### Tasks\n",
    "1. **Text Generation**: Generate text using a prompt\n",
    "2. **Fill-Mask**: Predict missing words (MLM task)\n",
    "3. **Question Answering**: Answer questions based on context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6416022",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnav\\AppData\\Roaming\\Python\\Python314\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Ready to run experiments...\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to run experiments...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc60538",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "**Task**: Try to generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
    "\n",
    "**Hypothesis**: Which models will fail? Why? Encoders cannot generate new tokens easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d4eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 1: TEXT GENERATION\n",
      "============================================================\n",
      "\n",
      "Prompt: 'The future of Artificial Intelligence is'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing BERT (bert-base-uncased):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran (expected failure for encoder-only).\n",
      "Output (truncated): The future of Artificial Intelligence is..............................\n"
     ]
    }
   ],
   "source": [
    "# Test Text Generation with BERT (encoder-only; expect poor generation)\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1: TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPrompt: 'The future of Artificial Intelligence is'\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BERT (bert-base-uncased):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    gen_bert = pipeline('text-generation', model='bert-base-uncased')\n",
    "    result_bert = gen_bert(\n",
    "        \"The future of Artificial Intelligence is\",\n",
    "        max_new_tokens=30,\n",
    "        truncation=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    bert_gen_output = result_bert[0]['generated_text'][:200]\n",
    "    bert_gen_status = \"Failure\"  # encoder-only models are not suited for generation\n",
    "    print(\"Ran (expected failure for encoder-only).\")\n",
    "    print(f\"Output (truncated): {bert_gen_output}\")\n",
    "except Exception as e:\n",
    "    bert_gen_status = \"Failure\"\n",
    "    bert_gen_output = str(e)[:200]\n",
    "    print(f\"FAILED: {bert_gen_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb86ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing RoBERTa (roberta-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran (expected failure for encoder-only).\n",
      "Output (truncated): The future of Artificial Intelligence is\n"
     ]
    }
   ],
   "source": [
    "# Test Text Generation with RoBERTa (encoder-only; expect poor generation)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing RoBERTa (roberta-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    gen_roberta = pipeline('text-generation', model='roberta-base')\n",
    "    result_roberta = gen_roberta(\n",
    "        \"The future of Artificial Intelligence is\",\n",
    "        max_new_tokens=30,\n",
    "        truncation=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    roberta_gen_output = result_roberta[0]['generated_text'][:200]\n",
    "    roberta_gen_status = \"Failure\"  # encoder-only models are not suited for generation\n",
    "    print(\"Ran (expected failure for encoder-only).\")\n",
    "    print(f\"Output (truncated): {roberta_gen_output}\")\n",
    "except Exception as e:\n",
    "    roberta_gen_status = \"Failure\"\n",
    "    roberta_gen_output = str(e)[:200]\n",
    "    print(f\"FAILED: {roberta_gen_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf0f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing BART (facebook/bart-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "Output (truncated): The future of Artificial Intelligence is Vaj Vaj Router Router Routerplin Router RouterABC Router Router bolst Router soul soul soulishers bolst bolst soul soul standby standby standby Router Router e\n"
     ]
    }
   ],
   "source": [
    "# Test Text Generation with BART (encoder-decoder; suitable for generation)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BART (facebook/bart-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    gen_bart = pipeline('text-generation', model='facebook/bart-base')\n",
    "    result_bart = gen_bart(\n",
    "        \"The future of Artificial Intelligence is\",\n",
    "        max_new_tokens=30,\n",
    "        truncation=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    bart_gen_output = result_bart[0]['generated_text'][:200]\n",
    "    bart_gen_status = \"Success\"\n",
    "    print(\"SUCCESS!\")\n",
    "    print(f\"Output (truncated): {bart_gen_output}\")\n",
    "except Exception as e:\n",
    "    bart_gen_status = \"Failure\"\n",
    "    bart_gen_output = str(e)[:200]\n",
    "    print(f\"FAILED: {bart_gen_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75904077",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling (Fill-Mask)\n",
    "\n",
    "**Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
    "\n",
    "**Hypothesis**: BERT/RoBERTa were trained on MLM. They should perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2090d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 2: MASKED LANGUAGE MODELING (FILL-MASK)\n",
      "============================================================\n",
      "\n",
      "Prompt: 'The goal of Generative AI is to [MASK] new content.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing BERT (bert-base-uncased):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "Top predictions:\n",
      "  1. 'create' (score: 0.5397)\n",
      "  2. 'generate' (score: 0.1558)\n",
      "  3. 'produce' (score: 0.0541)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 2: MASKED LANGUAGE MODELING (FILL-MASK)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPrompt: 'The goal of Generative AI is to [MASK] new content.'\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BERT (bert-base-uncased):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    mask_bert = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    result_mask_bert = mask_bert(\"The goal of Generative AI is to [MASK] new content.\")\n",
    "    print(\"SUCCESS!\")\n",
    "    print(\"Top predictions:\")\n",
    "    for i, pred in enumerate(result_mask_bert[:3], 1):\n",
    "        print(f\"  {i}. '{pred['token_str'].strip()}' (score: {pred['score']:.4f})\")\n",
    "    bert_mask_status = \"Success\"\n",
    "    bert_mask_output = f\"Top: {result_mask_bert[0]['token_str'].strip()}\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    bert_mask_status = \"Failure\"\n",
    "    bert_mask_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f6225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing RoBERTa (roberta-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "Top predictions:\n",
      "  1. 'generate' (score: 0.3711)\n",
      "  2. 'create' (score: 0.3677)\n",
      "  3. 'discover' (score: 0.0835)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing RoBERTa (roberta-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    mask_roberta = pipeline('fill-mask', model='roberta-base')\n",
    "    result_mask_roberta = mask_roberta(\"The goal of Generative AI is to <mask> new content.\")\n",
    "    print(\"SUCCESS!\")\n",
    "    print(\"Top predictions:\")\n",
    "    for i, pred in enumerate(result_mask_roberta[:3], 1):\n",
    "        print(f\"  {i}. '{pred['token_str'].strip()}' (score: {pred['score']:.4f})\")\n",
    "    roberta_mask_status = \"Success\"\n",
    "    roberta_mask_output = f\"Top: {result_mask_roberta[0]['token_str'].strip()}\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    roberta_mask_status = \"Failure\"\n",
    "    roberta_mask_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8257bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing BART (facebook/bart-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTED (but weak for MLM)\n",
      "Top predictions:\n",
      "  1. 'create' (score: 0.0746)\n",
      "  2. 'help' (score: 0.0657)\n",
      "  3. 'provide' (score: 0.0609)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BART (facebook/bart-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    mask_bart = pipeline('fill-mask', model='facebook/bart-base')\n",
    "    result_mask_bart = mask_bart(\"The goal of Generative AI is to <mask> new content.\")\n",
    "    print(\"EXECUTED (but weak for MLM)\")\n",
    "    print(\"Top predictions:\")\n",
    "    for i, pred in enumerate(result_mask_bart[:3], 1):\n",
    "        print(f\"  {i}. '{pred['token_str'].strip()}' (score: {pred['score']:.4f})\")\n",
    "    bart_mask_status = \"Failure (very weak)\"\n",
    "    bart_mask_output = f\"Top: {result_mask_bart[0]['token_str'].strip()}\" if result_mask_bart else \"No prediction\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    bart_mask_status = \"Failure (very weak)\"\n",
    "    bart_mask_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fa78a",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n",
    "\n",
    "**Task**: Answer the question `\"What are the risks?\"` based on the context:\n",
    "`\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
    "\n",
    "**Note**: Base models (not fine-tuned for SQuAD) may yield poor results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742a4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 3: QUESTION ANSWERING\n",
      "============================================================\n",
      "\n",
      "Question: 'What are the risks?'\n",
      "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing BERT (bert-base-uncased):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTED (base model, not fine-tuned for QA)\n",
      "Answer: 'risks such as hallucinations'\n",
      "Confidence: 0.0093\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 3: QUESTION ANSWERING\")\n",
    "print(\"=\" * 60)\n",
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "print(f\"\\nQuestion: '{question}'\")\n",
    "print(f\"Context: '{context}'\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BERT (bert-base-uncased):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    qa_bert = pipeline('question-answering', model='bert-base-uncased')\n",
    "    result_qa_bert = qa_bert(question=question, context=context)\n",
    "    print(\"EXECUTED (base model, not fine-tuned for QA)\")\n",
    "    print(f\"Answer: '{result_qa_bert['answer']}'\")\n",
    "    print(f\"Confidence: {result_qa_bert['score']:.4f}\")\n",
    "    bert_qa_status = \"Poor success\"\n",
    "    bert_qa_output = f\"Answer: {result_qa_bert['answer']} (score {result_qa_bert['score']:.3f})\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    bert_qa_status = \"Failure\"\n",
    "    bert_qa_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d70dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing RoBERTa (roberta-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTED (base model, not fine-tuned for QA)\n",
      "Answer: 'deepfakes.'\n",
      "Confidence: 0.0129\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing RoBERTa (roberta-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    qa_roberta = pipeline('question-answering', model='roberta-base')\n",
    "    result_qa_roberta = qa_roberta(question=question, context=context)\n",
    "    print(\"EXECUTED (base model, not fine-tuned for QA)\")\n",
    "    print(f\"Answer: '{result_qa_roberta['answer']}'\")\n",
    "    print(f\"Confidence: {result_qa_roberta['score']:.4f}\")\n",
    "    roberta_qa_status = \"Poor success\"\n",
    "    roberta_qa_output = f\"Answer: {result_qa_roberta['answer']} (score {result_qa_roberta['score']:.3f})\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    roberta_qa_status = \"Failure\"\n",
    "    roberta_qa_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0491287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Testing BART (facebook/bart-base):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTED (base model, not fine-tuned for QA)\n",
      "Answer: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes'\n",
      "Confidence: 0.0644\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing BART (facebook/bart-base):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    qa_bart = pipeline('question-answering', model='facebook/bart-base')\n",
    "    result_qa_bart = qa_bart(question=question, context=context)\n",
    "    print(\"EXECUTED (base model, not fine-tuned for QA)\")\n",
    "    print(f\"Answer: '{result_qa_bart['answer']}'\")\n",
    "    print(f\"Confidence: {result_qa_bart['score']:.4f}\")\n",
    "    bart_qa_status = \"Poor success\"\n",
    "    bart_qa_output = f\"Answer: {result_qa_bart['answer']} (score {result_qa_bart['score']:.3f})\"\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: {str(e)[:100]}\")\n",
    "    bart_qa_status = \"Failure\"\n",
    "    bart_qa_output = str(e)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d2553",
   "metadata": {},
   "source": [
    "## Summary: Observation Table\n",
    "\n",
    "Fill this table with your findings from the experiments above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a2727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FINAL MODEL BENCHMARK SUMMARY\n",
      "==========================================================================================\n",
      "      Task   Model          Result                                                Observation                                                          Why it happened\n",
      "Generation    BERT   Not Supported Generation not supported due to encoder-only architecture.  Encoder-only architecture; lacks decoder for autoregressive generation.\n",
      "Generation RoBERTa   Not Supported Generation not supported due to encoder-only architecture.    Encoder-only model; optimized for understanding, not text generation.\n",
      "Generation    BART         Success          Generated a text sequence using seq2seq decoding. Encoder–decoder architecture; explicitly trained for seq2seq generation.\n",
      " Fill-Mask    BERT         Success  Predicted masked token candidates with confidence scores.  Pretrained with Masked Language Modeling objective; strong predictions.\n",
      " Fill-Mask RoBERTa         Success  Predicted masked token candidates with confidence scores.            Improved MLM training with dynamic masking and larger corpus.\n",
      " Fill-Mask    BART Partial Success  Predicted masked token candidates with confidence scores.        Denoising autoencoder pretraining; MLM not its primary objective.\n",
      "        QA    BERT         Success        Produced an answer span with associated confidence.        Base model without QA fine-tuning; extraction quality is limited.\n",
      "        QA RoBERTa         Success        Produced an answer span with associated confidence.        Base model not trained on QA datasets; answers may be inaccurate.\n",
      "        QA    BART         Success        Produced an answer span with associated confidence.               Seq2seq base model; not optimized for extractive QA tasks.\n",
      "\n",
      "==========================================================================================\n",
      "ARCHITECTURAL OBSERVATIONS\n",
      "==========================================================================================\n",
      "\n",
      "KEY OBSERVATIONS:\n",
      "\n",
      "1. TEXT GENERATION:\n",
      "   - BERT and RoBERTa do not support generation due to encoder-only architecture.\n",
      "   - BART performs well as it is explicitly designed for seq2seq generation.\n",
      "\n",
      "2. FILL-MASK (MLM):\n",
      "   - BERT and RoBERTa show strong performance due to MLM-based pretraining.\n",
      "   - BART performs inconsistently as MLM is not its primary objective.\n",
      "\n",
      "3. QUESTION ANSWERING:\n",
      "   - All models can run QA pipelines.\n",
      "   - Base (non-finetuned) models yield weaker answers.\n",
      "   - Encoder-only models are more naturally suited for extractive QA after fine-tuning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getv(name, default=\"Not run\"):\n",
    "    val = globals().get(name, default)\n",
    "    return default if val is None else val\n",
    "\n",
    "\n",
    "# Clear, architecture-aware explanation per (task, model)\n",
    "WHY_MAP = {\n",
    "    (\"Generation\", \"BERT\"): \"Encoder-only architecture; lacks decoder for autoregressive generation.\",\n",
    "    (\"Generation\", \"RoBERTa\"): \"Encoder-only model; optimized for understanding, not text generation.\",\n",
    "    (\"Generation\", \"BART\"): \"Encoder–decoder architecture; explicitly trained for seq2seq generation.\",\n",
    "    \n",
    "    (\"Fill-Mask\", \"BERT\"): \"Pretrained with Masked Language Modeling objective; strong predictions.\",\n",
    "    (\"Fill-Mask\", \"RoBERTa\"): \"Improved MLM training with dynamic masking and larger corpus.\",\n",
    "    (\"Fill-Mask\", \"BART\"): \"Denoising autoencoder pretraining; MLM not its primary objective.\",\n",
    "    \n",
    "    (\"QA\", \"BERT\"): \"Base model without QA fine-tuning; extraction quality is limited.\",\n",
    "    (\"QA\", \"RoBERTa\"): \"Base model not trained on QA datasets; answers may be inaccurate.\",\n",
    "    (\"QA\", \"BART\"): \"Seq2seq base model; not optimized for extractive QA tasks.\",\n",
    "}\n",
    "\n",
    "ERROR_TOKENS = (\"FAILED\", \"error\", \"Exception\", \"Traceback\")\n",
    "\n",
    "\n",
    "def normalize_classification(task, model, status):\n",
    "    # Explicit architectural constraints\n",
    "    if task == \"Generation\" and model in (\"BERT\", \"RoBERTa\"):\n",
    "        return \"Not Supported\"\n",
    "    if task == \"Fill-Mask\" and model == \"BART\":\n",
    "        return \"Partial Success\"\n",
    "\n",
    "    s = (status or \"Not run\").lower()\n",
    "    if any(tok.lower() in s for tok in ERROR_TOKENS):\n",
    "        return \"Failure\"\n",
    "    if \"success\" in s:\n",
    "        return \"Success\"\n",
    "    if \"partial\" in s or \"weak\" in s or \"poor\" in s:\n",
    "        return \"Partial Success\"\n",
    "    return \"Not run\"\n",
    "\n",
    "\n",
    "def describe_output(task, model, output):\n",
    "    if task == \"Generation\" and model in (\"BERT\", \"RoBERTa\"):\n",
    "        return \"Generation not supported due to encoder-only architecture.\"\n",
    "    if output in (\"Not run\", \"Not captured\"):\n",
    "        return \"Task not executed or output not captured.\"\n",
    "    if any(tok.lower() in str(output).lower() for tok in ERROR_TOKENS):\n",
    "        return \"Execution resulted in an error.\"\n",
    "    if task == \"Generation\":\n",
    "        return \"Generated a text sequence using seq2seq decoding.\"\n",
    "    if task == \"Fill-Mask\":\n",
    "        return \"Predicted masked token candidates with confidence scores.\"\n",
    "    if task == \"QA\":\n",
    "        return \"Produced an answer span with associated confidence.\"\n",
    "    return \"Output recorded.\"\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    \"Generation\", \"Generation\", \"Generation\",\n",
    "    \"Fill-Mask\", \"Fill-Mask\", \"Fill-Mask\",\n",
    "    \"QA\", \"QA\", \"QA\",\n",
    "]\n",
    "\n",
    "models = [\n",
    "    \"BERT\", \"RoBERTa\", \"BART\",\n",
    "    \"BERT\", \"RoBERTa\", \"BART\",\n",
    "    \"BERT\", \"RoBERTa\", \"BART\",\n",
    "]\n",
    "\n",
    "raw_statuses = [\n",
    "    getv(\"bert_gen_status\"), getv(\"roberta_gen_status\"), getv(\"bart_gen_status\"),\n",
    "    getv(\"bert_mask_status\"), getv(\"roberta_mask_status\"), getv(\"bart_mask_status\"),\n",
    "    getv(\"bert_qa_status\"), getv(\"roberta_qa_status\"), getv(\"bart_qa_status\"),\n",
    "]\n",
    "\n",
    "classifications = [\n",
    "    normalize_classification(t, m, s)\n",
    "    for t, m, s in zip(tasks, models, raw_statuses)\n",
    "]\n",
    "\n",
    "raw_outputs = [\n",
    "    getv(\"bert_gen_output\", \"Not captured\"), getv(\"roberta_gen_output\", \"Not captured\"), getv(\"bart_gen_output\", \"Not captured\"),\n",
    "    getv(\"bert_mask_output\", \"Not captured\"), getv(\"roberta_mask_output\", \"Not captured\"), getv(\"bart_mask_output\", \"Not captured\"),\n",
    "    getv(\"bert_qa_output\", \"Not captured\"), getv(\"roberta_qa_output\", \"Not captured\"), getv(\"bart_qa_output\", \"Not captured\"),\n",
    "]\n",
    "\n",
    "observations = [\n",
    "    describe_output(t, m, o)\n",
    "    for t, m, o in zip(tasks, models, raw_outputs)\n",
    "]\n",
    "\n",
    "why_list = [\n",
    "    WHY_MAP.get((t, m), \"Not explained\")\n",
    "    for t, m in zip(tasks, models)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Task\": tasks,\n",
    "    \"Model\": models,\n",
    "    \"Result\": classifications,\n",
    "    \"Observation\": observations,\n",
    "    \"Why it happened\": why_list,\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"FINAL MODEL BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ARCHITECTURAL OBSERVATIONS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "analysis = \"\"\"\n",
    "KEY OBSERVATIONS:\n",
    "\n",
    "1. TEXT GENERATION:\n",
    "   - BERT and RoBERTa do not support generation due to encoder-only architecture.\n",
    "   - BART performs well as it is explicitly designed for seq2seq generation.\n",
    "\n",
    "2. FILL-MASK (MLM):\n",
    "   - BERT and RoBERTa show strong performance due to MLM-based pretraining.\n",
    "   - BART performs inconsistently as MLM is not its primary objective.\n",
    "\n",
    "3. QUESTION ANSWERING:\n",
    "   - All models can run QA pipelines.\n",
    "   - Base (non-finetuned) models yield weaker answers.\n",
    "   - Encoder-only models are more naturally suited for extractive QA after fine-tuning.\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
