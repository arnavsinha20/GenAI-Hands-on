{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1c69d711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c69d711",
        "outputId": "b945f604-3943-40b1-9068-4969cabcad1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bd53d7c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd53d7c6",
        "outputId": "7598018e-4479-4557-f665-b516b4bce2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GROQ API key: ··········\n",
            "✅ Groq client initialized securely\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "from getpass import getpass\n",
        "\n",
        "# Ask user to enter API key securely (hidden input)\n",
        "api_key = getpass(\"Enter your GROQ API key: \")\n",
        "\n",
        "# Create client\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "print(\"✅ Groq client initialized securely\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0a157dd0",
      "metadata": {
        "id": "0a157dd0"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = {\n",
        "    \"technical\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"system_prompt\": (\n",
        "            \"You are a rigorous technical support engineer. \"\n",
        "            \"Diagnose bugs, request minimal repro steps, and provide precise code fixes.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "\n",
        "    \"billing\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"system_prompt\": (\n",
        "            \"You are an empathetic billing support agent. \"\n",
        "            \"Help with refunds, subscriptions, payments and policies.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "\n",
        "    \"general\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"system_prompt\": (\n",
        "            \"You are a friendly general assistant who helps with everyday questions.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "857cdef7",
      "metadata": {
        "id": "857cdef7"
      },
      "outputs": [],
      "source": [
        "def route_prompt(user_input):\n",
        "\n",
        "    router_prompt = f\"\"\"\n",
        "Classify the user message into ONE category:\n",
        "technical\n",
        "billing\n",
        "general\n",
        "\n",
        "Return ONLY the category word.\n",
        "\n",
        "User message: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.0,   # deterministic routing\n",
        "        messages=[{\"role\":\"user\",\"content\":router_prompt}]\n",
        "    )\n",
        "\n",
        "    category = completion.choices[0].message.content.strip().lower()\n",
        "    return category"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_request(user_input):\n",
        "\n",
        "    # Step 1: Route query\n",
        "    category = route_prompt(user_input)\n",
        "    print(\"Routed to:\", category)\n",
        "\n",
        "    # Step 2: Load correct expert config\n",
        "    config = MODEL_CONFIG.get(category, MODEL_CONFIG[\"general\"])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # Step 3: Call expert LLM\n",
        "    completion = client.chat.completions.create(\n",
        "        model=config[\"model\"],\n",
        "        temperature=config[\"temperature\"],\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Step 4: Return response\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "HIoq4P2b9RnC"
      },
      "id": "HIoq4P2b9RnC",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = [\n",
        "    \"My Python script throws IndexError on line 5.\",\n",
        "    \"I was charged twice for my subscription.\",\n",
        "    \"Tell me a fun fact about space.\"\n",
        "]"
      ],
      "metadata": {
        "id": "nvF31UAA9Tml"
      },
      "id": "nvF31UAA9Tml",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in test_inputs:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"User:\", text)\n",
        "\n",
        "    response = process_request(text)\n",
        "\n",
        "    print(\"\\nAssistant:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO1LcwXj9Vdl",
        "outputId": "5965cf7f-40b9-43aa-8651-9474e3d6300d"
      },
      "id": "hO1LcwXj9Vdl",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "User: My Python script throws IndexError on line 5.\n",
            "Routed to: technical\n",
            "\n",
            "Assistant: To better assist you, I'll need more information about the error you're encountering.\n",
            "\n",
            "1. **Error Message**: Can you please provide the full error message you're seeing, including the `IndexError` exception?\n",
            "2. **Code Snippet**: Please share the code surrounding line 5, ideally 5-10 lines before and after line 5. This will help me understand the context of the error.\n",
            "3. **Input/Context**: What input or data is your script processing when the error occurs?\n",
            "\n",
            "Once I have this information, I can provide a more accurate diagnosis and guidance on how to resolve the `IndexError`.\n",
            "\n",
            "==============================\n",
            "User: I was charged twice for my subscription.\n",
            "Routed to: billing\n",
            "\n",
            "Assistant: I'm so sorry to hear that you were charged twice for your subscription. That can be really frustrating and I'm here to help resolve the issue for you.\n",
            "\n",
            "Can you please provide me with some more information about the duplicate charge? For example, what is the date of the charges, and what is the amount that was deducted from your account each time? Additionally, could you please confirm your subscription details, such as the type of subscription you have and the payment method you used?\n",
            "\n",
            "Once I have this information, I'll do my best to investigate the issue and work with you to process a refund for the duplicate charge as soon as possible. Your satisfaction is important to me, and I want to make sure that you're taken care of.\n",
            "\n",
            "==============================\n",
            "User: Tell me a fun fact about space.\n",
            "Routed to: general\n",
            "\n",
            "Assistant: Here's a fun fact: Did you know that there's a giant storm on Jupiter that has been raging for at least 150 years? It's called the Great Red Spot, and it's so large that three Earths could fit inside it. The storm is a giant anticyclonic storm, meaning that it's a high-pressure region with clockwise rotation, and it's been continuously raging since it was first observed in 1831. Isn't that mind-blowing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bitcoin_price_tool():\n",
        "    return \"Bitcoin price is approximately $60,000 (mock data).\"\n",
        "\n",
        "def route_prompt(user_input):\n",
        "\n",
        "    router_prompt = f\"\"\"\n",
        "Classify the user message into ONE category:\n",
        "technical\n",
        "billing\n",
        "general\n",
        "tool\n",
        "\n",
        "Return ONLY the category word.\n",
        "\n",
        "User message: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.0,\n",
        "        messages=[{\"role\":\"user\",\"content\":router_prompt}]\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content.strip().lower()"
      ],
      "metadata": {
        "id": "lxATE5lJ9XQw"
      },
      "id": "lxATE5lJ9XQw",
      "execution_count": 39,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}